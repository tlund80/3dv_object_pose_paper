\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{3dv-2015authorkit/latex/3dv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\threedvfinalcopy % *** Uncomment this line for the final submission

\def\threedvPaperID{****} % *** Enter the 3DV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifthreedvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{A Large-Scale 3D Object Recognition dataset}

%\author{Thomas S{\o}lund\\
%Institution1\\
%Institution1 address\\
%{\tt\small thso@dti.dk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Anders Glent Buch, Norbert Kr\"u{}ger\\
%M\ae{}rsk Mc-Kinney M\o{}ller Institute,\\
%University of Southern Denmark,\\
%DK-5230 Odense, Denmark \\
%{\tt\small anbu,norbert@mmmi.sdu.dk}
%\and
%Knut Conradsen, Henrik Aan√¶s\\
%Department of Applied Mathematics\\ and Computer Science,\\ 
%Technical University of Denmark\\
%DK-2800 Kgs. Lyngby, Denmark\\
%{\tt\small knco,aanes@dtu.dk}
%}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 To be written...........
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The ability to recognise objects in range images is a fundamental research area in computer vision with many different application areas. With continually introduction of new affordable 3D sensors for different applications, the ability to localize and recognize rigid and no rigid objects are a attractive and unavoidable technology in the future. Applications areas such as robotic assembly, bin-picking, mobile manipulation, biometric analysis, tracking and intelligent surveillance all benefits from 3D data for recognizing and localizing objects. Mainly, because the third dimension are explicit given and not inferred as known from 2D object recognition and pose estimation. In the last decades a huge effort in 2D object recognition and classification has been going on, where methods have been evaluated on large-scale dataset like the PASCAL Visual Object Challenge (VOC) \cite{Everingham2014}, ImageNet \cite{Imagenet2009}. The ability to evaluate on a large-scale dataset has proven valuable in the future algorithm development after the release of the PASCAL VOC and ImageNet datasets \cite{Everingham2014}. In 3D object recognition and pose estimation there still lacks a large-scale dataset in order to evaluate algorithms. Until today 3D object recognition and pose estimations has been evaluated at a couple of smaller datasets namely the UWA \cite{Mian2006}, Queens \cite{Taati2007}, Bologna \cite{Salti2014},\cite{Tombari2010}, Vienna Kinect\cite{Aldoma2012}, TUM \cite{Rodola2013}, TUM-LineMod\cite{Hinterstoisser2012}, BigBird\cite{BigBIRD} and RGB-D dataset version 1 \& 2\cite{Lai2011}, \cite{Lai2014}.
In this work we present a evaluation of local shape features and present a new large-scale dataset consisting of 50 objects and 300 scenes. The new dataset is recorded systematically using a setup with a industrial robot, a structured light sensor and a Kinect V2 sensor, which records data from 11 different view points. Each scene consists of 10 objects  automatic take with two different sensors, which results in a dataset with around 66000 unique object poses. 


%and pose estimation \cite{Viksten09}, \cite{Hinterstoisser2012}, this kind of scientific validations of results still lacks in 3D object recognition. 
%2D object recognition is well studied lacking with 3D. Keypoint comparison Aan{\ae}s et.al. \cite{Aanaes2011}

%We study the performance of the top six local shape features and one global descriptor 
%Limitations in current dataset and algorithms. Feature rich objects not flat or cylindrical objects



	

%The UWA dataset by Mian et.al. \cite{Mian2006}, \cite{Mian2010} is one of the best known datasets for 3D pose estimation algorithm evaluation. The dataset is composed of four full object models and 50 real scenes captures with a laser scanner which containing incomplete instances of the objects. Each scene contains three or four object models which results in much clutter and occlusion. The amount of occlusion and clutter in each scene is not provided. The four object models are generated with a multi-view registration algorithm. Each object model contain more than 100000 mesh vertices and posses good and distinctive shape features with non-uniform surfaces and ambiguities. The object models and scenes has pre-estimated normals and no color/texture. The dataset contains full 6D ground truth poses for each object as the only evaluation data type.

%The Queens Lidar dataset \cite{Taati2011}, \cite{Taati2007} is constructed like the UWA dataset with a laser scanner and consists of five object models and 80 real scenes take from one viewpoint. Like the UWA dataset, the Queens Lidar dataset contains no color information but the scenes has larger variation compared to the UWA dataset with one to five object models present in the scenes. The five object models has pre-estimated normals but is only provided as a point representation. The dataset contains full 6D ground truth poses for each object as the only evaluation data.

%The Queens Stereo dataset \cite{Taati2011}, \cite{Taati2007} contains five full object models and 100 real scenes created with a stereo camera and standard stereo reconstruction. The dataset is taken from one viewpoint. Each scene contains three object models with no color, normal, occlusion or clutter information available. Like the Queens Lidar dataset the object models is represented with points and no vertices. The dataset contains full 6D ground truth poses for each object as the only evaluation data.

%The Bologna 1 \& 2 dataset \cite{Salti2014}, \cite{Tombari2010} contains six full object models and 45 synthetic scenes. All scenes is construct a synthetic scenes with models from the Stanford 3D Scanning Repository. Each 45 scenes are created by applying a random transformation to a random subset of object model and store the scene. The difference between the Bologna 1 and Bologna 2 dataset are that Bologna 2 is re-sampled 1/8 of the original point density. The fact that it is a synthetic dataset results in 100 percent accurate 6D ground truth data.
%The Bologna 3 dataset \cite{Salti2014}, \cite{Tombari2010} contains eight models and 15 scenes. Each scenes are acquired with spacetime stereo technique and contains both color and surface normal information. Each scene contains two object models and additional objects to create clutter and occlusion. The eight models are single view object templates with color and estimated normals in mesh representation. The dataset contains full 6D pose estimates as ground truth. Their exist no data on occlusion and clutter estimates.
%The Bologna 4 dataset \cite{Salti2014}, \cite{Tombari2010} contains eight models and 16 scenes. As Bologna 3, this dataset is acquired with spacetime stereo techniques and containing color and normal data for both the object models and the scenes. The dataset includes object models with highly similar shapes but different textures. Each scene contains two object models and additional objects to create clutter and occlusion. The eight models are single view object templates with color and estimated normals in mesh representation. The dataset contains full 6D pose estimates as ground truth. Their exist no data on occlusion and clutter estimates.
%The Bologna 5 dataset \cite{Salti2014}, \cite{Tombari2010} contains six object models and 16 scenes. The dataset is acquired with a Microsoft Kinect sensor and includes color and normal information for both the object models and scenes. The provided object models is a set of object templates taken from different views. With a proper mesh registration algorithm a full object model can be created. The dataset contains full 6D pose estimates as ground truth. Their exist no data on occlusion and clutter estimates.
%The Vienna Kinect dataset \cite{Aldoma2012} contains 35 full object models and 50 scenes. Each scene is acquired with a Microsoft Kinect sensor and includes color information but no pre-estimated normals. All scenes is table top scenes with limited occlusion and clutter. The object models in each scene is distThe 35 object models are full represented mesh models without color but includes pre-estimated normals. 
%The dataset contains full 6D pose estimates as ground truth. Their exist no data on occlusion and clutter estimates.
%TUM dataset \cite{Rodola2013}\\
%RGB-D Dataset V1 \cite{Lai2011}, \cite{Lai2012}\\ 
%RGB-D Dataset V2 \cite{Lai2014}\\ 

%-------------------------------------------------------------------------
\section{Related work}
%We are not considering learning approaches
\textbf{Local Shape features:}\\


Spin Images \cite{Johnson1999}\\
Froome \etal \cite{Frome2004} presented a local descriptor 3D shape context.
USC \cite{usc2010}\\


FPFH \cite{Fpfh2009}\\
ESAC \cite{Ecsad2015}\\
ROPS \cite{Guo2013}\\
SHOT \cite{Salti2014}\\
NDHIST\\
PPF \cite{Drost2010}\\
SHOT \cite{Tombari2010}\\

Feature Fusion \cite{Buch2016}\\
Feature comparision Yulan et.al. \cite{Guo2015}

%------------------------------------------------------------------------
%\section{Experimental design}

%-------------------------------------------------------------------------
%\section{Benchmark}

%-------------------------------------------------------------------------
%\section{Evaluation}

%-------------------------------------------------------------------------
%\section{Discussion}

%-------------------------------------------------------------------------
%\section{Conclusion}

%-------------------------------------------------------------------------

{\small
\bibliographystyle{3dv-2015authorkit/latex/ieee}
\bibliography{bib}
}
\end{document}